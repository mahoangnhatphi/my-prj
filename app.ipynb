{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtW2Y0SxCXx9qcPqjG9HcX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahoangnhatphi/prj/blob/main/app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8kqk-0bdNQS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 0: Import Datasets"
      ],
      "metadata": {
        "id": "b6Y1SOPwdNvG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l3LqVFKsdQpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5tybxxcp2IK",
        "outputId": "e40f7ae1-fd7f-4153-ccf5-cfca1d50eca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "# https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\n",
        "# https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip\n",
        "\n",
        "import requests, zipfile, io, os, shutil\n",
        "\n",
        "root = '/content'\n",
        "dogimages_url = \"https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\"\n",
        "humanimages_url = \"https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip\"\n",
        "\n",
        "data_path = os.path.join(root,'data')\n",
        "dogimages_path = os.path.join(data_path,'dogImages')\n",
        "humanimages_path = os.path.join(data_path,'lfw')\n",
        "data_path = os.path.join(root,'data')\n",
        "if not os.path.exists(data_path): os.mkdir(data_path)\n",
        "\n",
        "# if os.path.exists(dogimages_path):shutil.rmtree(dogimages_path)\n",
        "# if os.path.exists(humanimages_path):shutil.rmtree(humanimages_path)\n",
        "\n",
        "if not os.path.exists(dogimages_path):\n",
        "  print(\"downloading dog images\")\n",
        "  r = requests.get(dogimages_url)\n",
        "  z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "  z.extractall(data_path)\n",
        "\n",
        "if not os.path.exists(humanimages_path):\n",
        "  print(\"downloading human images\")\n",
        "  r = requests.get(humanimages_url)\n",
        "  z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "  z.extractall(data_path)\n",
        "\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "# load filenames for human and dog images\n",
        "human_files = np.array(glob(\"/content/data/lfw/*/*\"))\n",
        "dog_files = np.array(glob(\"/content/data/dogImages/*/*/*\"))\n",
        "\n",
        "# print number of images in each dataset\n",
        "print('There are %d total human images.' % len(human_files))\n",
        "print('There are %d total dog images.' % len(dog_files))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading dog images\n",
            "downloading human images\n",
            "There are 13233 total human images.\n",
            "There are 8351 total dog images.\n",
            "CPU times: user 10.3 s, sys: 4.56 s, total: 14.9 s\n",
            "Wall time: 38.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('/content/haarcascades'): os.mkdir('/content/haarcascades')\n",
        "haarcascades_url = 'https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_alt.xml'\n",
        "!wget -O /content/haarcascades/haarcascade_frontalface_alt.xml {haarcascades_url}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHTIp55EdV9G",
        "outputId": "73c043a7-b354-44a0-b956-8cad38739b44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-01 11:17:13--  https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_alt.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 676709 (661K) [text/plain]\n",
            "Saving to: ‘/content/haarcascades/haarcascade_frontalface_alt.xml’\n",
            "\n",
            "/content/haarcascad 100%[===================>] 660.85K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-09-01 11:17:13 (14.4 MB/s) - ‘/content/haarcascades/haarcascade_frontalface_alt.xml’ saved [676709/676709]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Detect Dogs"
      ],
      "metadata": {
        "id": "h2Ubjm_rdh42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Dog Detector"
      ],
      "metadata": {
        "id": "uoQ0gXp0dli5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtain Pre-trained VGG-16 Model\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# define VGG16 model\n",
        "VGG16 = models.vgg16(pretrained=True)\n",
        "\n",
        "# check if CUDA is available\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "# move model to GPU if CUDA is available\n",
        "if use_cuda:\n",
        "    VGG16 = VGG16.cuda()"
      ],
      "metadata": {
        "id": "c__Cz5bTd1du"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "import os\n",
        "\n",
        "def VGG16_predict(img_path):\n",
        "    '''\n",
        "    Use pre-trained VGG-16 model to obtain index corresponding to\n",
        "    predicted ImageNet class for image at specified path\n",
        "\n",
        "    Args:\n",
        "        img_path: path to an image\n",
        "\n",
        "    Returns:\n",
        "        Index corresponding to VGG-16 model's prediction\n",
        "    '''\n",
        "\n",
        "    ## TODO: Complete the function.\n",
        "    ## Load and pre-process an image from the given img_path\n",
        "    ## Return the *index* of the predicted class for that image\n",
        "\n",
        "    # data transformation\n",
        "    batch_size = 64\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    image_transforms = transforms.Compose([\n",
        "                            transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "                            transforms.RandomRotation(degrees=15),\n",
        "                            transforms.ColorJitter(),\n",
        "                            transforms.RandomHorizontalFlip(),\n",
        "                            transforms.CenterCrop(size=224),  # Image net standards\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                 [0.229, 0.224, 0.225])  # Imagenet standards\n",
        "                        ])\n",
        "\n",
        "    image = image_transforms(img)[:3,:,:].unsqueeze(0)\n",
        "#     image = image_transforms(img)\n",
        "#     print(image_transformation)\n",
        "\n",
        "    if use_cuda:\n",
        "        image = image.cuda()\n",
        "    output = VGG16(image)\n",
        "\n",
        "    _,pred = torch.max(output, dim=1)\n",
        "    pred=pred.cpu()\n",
        "    pred = pred.data.numpy()[0]\n",
        "\n",
        "    return pred # predicted class index\n",
        "\n",
        "VGG16_predict(dog_files[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v_9IoitdwkW",
        "outputId": "d66067ab-1b60-49d3-9fe7-d255738b3ee4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "189"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### returns \"True\" if a dog is detected in the image stored at img_path\n",
        "def dog_detector(img_path):\n",
        "    ## TODO: Complete the function.\n",
        "#     in VGG16 index 151 to 268 are dog classifications\n",
        "\n",
        "    return VGG16_predict(img_path)>= 151 and VGG16_predict(img_path)<=268 # true/false\n",
        "\n",
        "print(dog_detector(dog_files[7]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO9z0j54doNw",
        "outputId": "86d12219-3911-49d7-cf77-2efb8d5afc08"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assess the Dog Detector"
      ],
      "metadata": {
        "id": "YlsfXrN_d9if"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### TODO: Test the performance of the dog_detector function\n",
        "### on the images in human_files_short and dog_files_short.\n",
        "\n",
        "human_files_count=0\n",
        "dog_files_count=0\n",
        "for i in range(len(human_files_short)):\n",
        "    if dog_detector(human_files_short[i]):\n",
        "        human_files_count+=1\n",
        "    if dog_detector(dog_files_short[i]):\n",
        "        dog_files_count+=1\n",
        "print(\"number of dog faces detected in human_fiiles_short {0}\".format(human_files_count))\n",
        "print(\"number of dog faces detected in dog_files_short {0}\".format(dog_files_count))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "CS8MsYhOeCp2",
        "outputId": "00744714-e240-4e8c-9419-a109e0d1dfcb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6f74b94be784>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhuman_files_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdog_files_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuman_files_short\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdog_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuman_files_short\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mhuman_files_count\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'human_files_short' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_path = '/content/data/dogImages/train/'\n",
        "val_path = '/content/data/dogImages/valid'\n",
        "test_path = '/content/data/dogImages/test'\n",
        "\n",
        "batch_size=64\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_path,\n",
        "        # '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(150, 150),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        val_path,\n",
        "        # '/content/cats_dogs/data/val',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n",
        "test_generator = val_datagen.flow_from_directory(\n",
        "        test_path,\n",
        "        # '/content/cats_dogs/data/val',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKcHQv45eHvm",
        "outputId": "776dbb43-1e5e-468e-f288-289fee90a119"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6680 images belonging to 133 classes.\n",
            "Found 835 images belonging to 133 classes.\n",
            "Found 836 images belonging to 133 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), input_shape=( 150, 150, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "# model.add(Conv2D(64, (3, 3)))\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(133))\n",
        "model.add(Activation('softmax'))"
      ],
      "metadata": {
        "id": "IzZdoWpgeLpv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify Loss Function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Okg63Vg5ePi2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and Validate the model"
      ],
      "metadata": {
        "id": "wxwYeiyyeXvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        #steps_per_epoch=18631 // batch_size,\n",
        "        epochs=16,\n",
        "        validation_data=validation_generator,\n",
        "        #validation_steps=10119 // batch_size\n",
        "        )\n",
        "model.save_weights('first_try.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7hDYjUCeb_o",
        "outputId": "6708ddf6-5257-4c35-f600-3acf29550207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-919974f827fb>:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/16\n",
            " 19/105 [====>.........................] - ETA: 4:18 - loss: 4.8932 - accuracy: 0.0074"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the Model"
      ],
      "metadata": {
        "id": "JW0AN6mweeIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_prediction = model.predict(test_generator)\n",
        "model.metrics_names"
      ],
      "metadata": {
        "id": "j1uOJyyJeeuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import accuracy_score\n",
        "# model.evaluate_generator(test_generator)\n",
        "test_generator.reset()\n",
        "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
        "test_eval = model.evaluate_generator(test_generator,STEP_SIZE_TEST)\n",
        "print('test loss ',test_eval[0])\n",
        "print('test accuracy ',test_eval[1])"
      ],
      "metadata": {
        "id": "jH_ZKw5AeglX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}